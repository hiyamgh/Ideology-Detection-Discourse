#!/bin/bash -l

#SBATCH --job-name=mlm # Job name
#SBATCH --partition=gpu # Partition
#SBATCH --nodes=6 # Number of nodes
#SBATCH --gres=gpu:1 # Number of GPUs
#SBATCH --ntasks-per-node=1  # Number of tasks
#SBATCH --output=%j.out # Stdout (%j=jobId)
#SBATCH --error=%j.err # Stderr (%j=jobId)
#SBATCH --mem=180000
#SBATCH --time=23:00:00 # Walltime
#SBATCH -A p118

#Load any necessary modules, in this case OpenMPI with CUDA
module purge
module load Java/11.0.16
module load Python/3.10.4-GCCcore-11.3.0
module load PyTorch/1.12.0-foss-2022a-CUDA-11.7.0

#Mitigate fragmentation
#export PYTORCH_CUDA_ALLOC_CONF=garbage_collection_threshold:0.6,max_split_size_mb:128
export MASTER_ADDR=$(scontrol show hostname $SLURM_NODELIST | head -n1)
export MASTER_PORT=29500
export NODE_RANK=$SLURM_NODEID

TORCHELASTIC_ERROR_VERBOSE=1

cd /nvme/h/lb21hg1/data_p118/POST-THESIS/LLMs/

srun --mpi=pmi2 python -m torch.distributed.run --rdzv_backend=c10d --rdzv_endpoint=$MASTER_ADDR --nproc_per_node=$SLURM_NTASKS_PER_NODE --nnodes=$SLURM_NNODES --node_rank=$SLURM_NODEID --master_addr=$MASTER_ADDR --master_port=$MASTER_PORT test.py
